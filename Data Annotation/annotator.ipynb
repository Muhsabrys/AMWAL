{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Annotator\n",
    "\n",
    "(C) 2024 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "This is an annotator based on the spaCy pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"fill-mask\", model=\"aubmindlab/bert-large-arabertv02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_transformers import Transformer\n",
    "from spacy_transformers.pipeline_component import DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tokenizer_exceptions(nlp: spacy.language.Language, file_name: str):\n",
    "    with open(file_name, mode='r', encoding='utf-8') as ifp:\n",
    "        data = json.load(ifp)\n",
    "    for line in data:\n",
    "        nlp.tokenizer.add_special_case(line, [ {spacy.attrs.ORTH: f\"{i}\"} for i in data[line] ])\n",
    "\n",
    "def set_lemmatization_exceptions(nlp: spacy.language.Language, file_name: str):\n",
    "    data = {}\n",
    "    with open(file_name, mode='r', encoding='utf-8') as ifp:\n",
    "        data = json.load(ifp)\n",
    "    ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "    for token in data:\n",
    "        ruler.add(patterns=[[{\"TEXT\": f\"{token.strip()}\"}]], attrs={\"LEMMA\": f\"{data[token].strip()}\"})\n",
    "\n",
    "def load_span_entities(nlp: spacy.language.Language, file_name: str):\n",
    "    if file_name.endswith(\".xlsx\"):\n",
    "        rules = compile_pattern(nlp, file_name)\n",
    "    else:\n",
    "        rules = compile_pattern_json(file_name)\n",
    "    return rules\n",
    "\n",
    "def compile_pattern(nlp: spacy.language.Language, file_name: str) -> list:\n",
    "    entities = {}\n",
    "    df = pd.read_excel(file_name, index_col=None) # , header=None)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    ts = list(df)\n",
    "    for t in ts:\n",
    "        data = [ x.strip() for x in [ str(y) for y in df[t].tolist() ] if x.strip() ]\n",
    "        for entity in data:\n",
    "            val = entities.get(entity, set())\n",
    "            val.add(t)\n",
    "            entities[entity] = val\n",
    "    ne_rules = []\n",
    "    entity_keys = list(entities.keys())\n",
    "    for i in tqdm(range(len(entity_keys))):\n",
    "        a = entity_keys[i]\n",
    "        doc = nlp(a)\n",
    "        tokens = [ x.text for x in doc ]\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == u'\"':\n",
    "                tokens[i] = u\"\\\"\"\n",
    "        #tokens_lower_rules = [ {u\"LOWER\": lo} for lo in [ x.lower() for x in tokens ] ]\n",
    "        tokens_text_rules  = [ {u\"TEXT\":  t}  for t  in tokens ]\n",
    "        for c in entities[a]:\n",
    "            ne_rules.append({u\"label\": c, u\"pattern\": tokens_text_rules})\n",
    "            #ne_rules.append({u\"label\": c, u\"pattern\": tokens_lower_rules })\n",
    "    return ne_rules\n",
    "\n",
    "def compile_pattern_json(file_name: str) -> list:\n",
    "    with open(file_name, mode='r', encoding='utf-8') as ifp:\n",
    "        data = json.load(ifp)\n",
    "    return data[\"pattern\"]\n",
    "\n",
    "def load_nlp() -> spacy.language.Language:\n",
    "    nlp = spacy.blank(\"ar\")\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
    "            \"name\": \"aubmindlab/bert-large-arabertv02\"\n",
    "        }\n",
    "    }\n",
    "    nlp.add_pipe(\"transformer\", config=config)\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    df = pd.read_excel(\"NEWNER.xlsx\", index_col=None)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    ne_classes = {}\n",
    "    ts = list(df)\n",
    "    for t in ts:\n",
    "        data = [ x.strip() for x in [ str(y) for y in df[t].tolist() ] if x.strip() ]\n",
    "        for entity in data:\n",
    "            val = ne_classes.get(t, set())\n",
    "            val.add(entity)\n",
    "            ne_classes[t] = val\n",
    "    for nec in ne_classes:\n",
    "        terms = list(ne_classes[nec])\n",
    "        patterns = [ nlp.make_doc(text) for text in terms ]\n",
    "        matcher.add(nec, patterns)\n",
    "    if \"attribute_ruler\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"attribute_ruler\")\n",
    "    set_tokenizer_exceptions(nlp, \"tokenizer_exceptions.json\")\n",
    "    set_lemmatization_exceptions(nlp, \"lemmatization_exceptions.json\")\n",
    "    nlp.initialize()\n",
    "    return nlp, matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(text: str, nlp, matcher) -> list:\n",
    "    doc = nlp(text)\n",
    "    res = []\n",
    "    s_counter = 0\n",
    "    for s in doc.sents:\n",
    "        s_counter += 1\n",
    "        for token in s:\n",
    "            res.append( [ s_counter, token.i, token.text, 'O'] )\n",
    "    ents = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        match_id_string = nlp.vocab.strings[match_id]\n",
    "        ents.append( (span.text, match_id_string, start, end) )\n",
    "    return {\"tokens\": res, \"entities\": ents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp, matcher = load_nlp()\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for root, dirs, files in os.walk('Data/Jaz/txt'):\n",
    "    for f in files:\n",
    "        if f.endswith(\".txt\"):\n",
    "            file_list.append(os.path.join(root, f))\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    ofname = file_list[i][:-4]+\".json\"\n",
    "    if os.path.exists(ofname):\n",
    "        continue\n",
    "    with open(file_list[i], mode='r', encoding='utf-8') as ifp:\n",
    "        text = ifp.read()\n",
    "    res = process_file(text, nlp, matcher)\n",
    "    with open(ofname, mode='w', encoding='utf-8') as ofp:\n",
    "        ofp.write(json.dumps(res, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
